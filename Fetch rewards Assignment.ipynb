{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Rewards Coding Exercise - Text Similarity\n",
    "\n",
    "Sample Inputs are as follows:\n",
    "<br>\n",
    "Sample 1\n",
    "The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\n",
    "\n",
    "Sample 2\n",
    "The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\n",
    "\n",
    "Sample 3\n",
    "We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\n",
    "\n",
    "Formulation of the code is as follows:\n",
    "1. Reading/Loading the text\n",
    "2. Converting it to lowercase\n",
    "3. Tokenizing the words\n",
    "4. Removing the Stop Words\n",
    "5. Removing the punctuation\n",
    "6. Performing differnt methods for document similarity like Jaccard Similarity and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "doc2 = \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\n",
    "doc3 = \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\"\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(doc1, doc2):\n",
    "    ## Converting all the characters to lower case letters\n",
    "    doc1 = doc1.lower()\n",
    "    doc2 = doc2.lower()\n",
    "    \n",
    "    ## Tokeinizing the words using the self written tokkenize function\n",
    "    doc1 = tokkenize(doc1)\n",
    "    doc2 = tokkenize(doc2)\n",
    "    \n",
    "    ## Filtering the stopwords from the Tokenined words\n",
    "    filtered_doc1 = [word for word in doc1 if word not in stopwords]\n",
    "    filtered_doc2 = [word for word in doc2 if word not in stopwords]\n",
    "    \n",
    "    ## Removing the punctuation from the Filtered documents\n",
    "    remove_punct_doc1 = [''.join(c for c in s if c not in string.punctuation) for s in filtered_doc1]\n",
    "    remove_punct_doc2 = [''.join(c for c in s if c not in string.punctuation) for s in filtered_doc2]\n",
    "   \n",
    "   \n",
    "    return remove_punct_doc1,remove_punct_doc2\n",
    "\n",
    "def tokkenize(doc):\n",
    "    words = doc.split(' ')\n",
    "\n",
    "    words_in_sentences = [sentence.split(' ') for sentence in words]\n",
    "    return words_in_sentences\n",
    "\n",
    "def calculate_jaccard(word_tokens1, word_tokens2):\n",
    "    both_tokens = word_tokens1 + word_tokens2\n",
    "    union = set(both_tokens)\n",
    "\n",
    "    intersection = set()\n",
    "    for w in word_tokens1:\n",
    "        if w in word_tokens2:\n",
    "            intersection.add(w)\n",
    "\n",
    "    jaccard_score = len(intersection)/len(union)\n",
    "    return jaccard_score\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    return Counter(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity Between Sample First and Second:  0.6333333333333333\n",
      "Cosine Simiarity Between Sample First and Second:  0.8775496206997365\n",
      "\n",
      "Jaccard Similarity Between Sample First and Third:  0.16470588235294117\n",
      "Cosine Simiarity Between Sample First and Third:  0.5479591080851048\n"
     ]
    }
   ],
   "source": [
    "document1, document2 = preProcessing(doc1, doc2)\n",
    "print('Jaccard Similarity Between Sample First and Second: ',calculate_jaccard(document1,document2))\n",
    "print('Cosine Simiarity Between Sample First and Second: ',get_cosine(text_to_vector(document1), text_to_vector(document2)))\n",
    "\n",
    "\n",
    "document1, document3 = preProcessing(doc1, doc3)\n",
    "print('\\nJaccard Similarity Between Sample First and Third: ',calculate_jaccard(document1,document3))\n",
    "print('Cosine Simiarity Between Sample First and Third: ',get_cosine(text_to_vector(document1), text_to_vector(document3)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
